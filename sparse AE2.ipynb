{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an example of using Tensorflow to build Sparse Autoencoder\n",
    "# for representation learning.\n",
    "# It is the implementation of the sparse autoencoder for\n",
    "#        https://web.stanford.edu/class/cs294a/sparseAutoencoder_2011new.pdf\n",
    "#\n",
    "# For any enquiry, please contact Dr. Zhiwei Lin  at Ulster University\n",
    "#       http://scm.ulster.ac.uk/zhiwei.lin/\n",
    "#\n",
    "#\n",
    "# ==============================================================================\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardSparseAutoEncoder():\n",
    "    '''\n",
    "      This is the implementation of the sparse autoencoder for https://web.stanford.edu/class/cs294a/sparseAutoencoder_2011new.pdf\n",
    "    '''\n",
    "    def __init__(self, n_input, n_hidden,  rho=0.01, alpha=0.0001, beta=3, \n",
    "                 activation=tf.nn.adam, activation1=tf.nn.sigmoid, optimizer=tf.train.AdamOptimizer()):\n",
    "        self.n_input=n_input\n",
    "        self.n_hidden3=n_hidden\n",
    "        self.n_hidden2=64\n",
    "        self.n_hidden1=500\n",
    "        self.rho=rho  # sparse parameters\n",
    "        self.alpha =alpha\n",
    "        self.beta=beta\n",
    "        self.optimizer=optimizer\n",
    "        self.activation = activation\n",
    "\n",
    "        self.W1=self.init_weights((self.n_input,self.n_hidden1))\n",
    "        self.b1=self.init_weights((1,self.n_hidden1))\n",
    "        \n",
    "        self.W2=self.init_weights((self.n_hidden1,self.n_hidden2))\n",
    "        self.b2=self.init_weights((1,self.n_hidden2))\n",
    "        \n",
    "        self.W3=self.init_weights((self.n_hidden2,self.n_hidden3))\n",
    "        self.b3=self.init_weights((1,self.n_hidden3))\n",
    "        \n",
    "        self.W4=self.init_weights((self.n_hidden3,self.n_hidden2))\n",
    "        self.b4=self.init_weights((1,self.n_hidden2))\n",
    "        \n",
    "        self.W5=self.init_weights((self.n_hidden2,self.n_hidden1))\n",
    "        self.b5=self.init_weights((1,self.n_hidden1))\n",
    "\n",
    "        self.W6=self.init_weights((self.n_hidden1,self.n_input))\n",
    "        self.b6= self.init_weights((1,self.n_input))\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(init)\n",
    "\n",
    "    def init_weights(self,shape):\n",
    "        r= math.sqrt(6) / math.sqrt(self.n_input + self.n_hidden + 1)\n",
    "        weights = tf.random_normal(shape, stddev=r)\n",
    "        return tf.Variable(weights)\n",
    "\n",
    "    def encode(self,X):\n",
    "        l=tf.matmul(X, self.W1)+self.b1\n",
    "        return self.activation(l)\n",
    "\n",
    "    def decode(self,H):\n",
    "        l=tf.matmul(H,self.W2)+self.b2\n",
    "        return self.activation(l)\n",
    "\n",
    "\n",
    "    def kl_divergence(self, rho, rho_hat):\n",
    "        return rho * tf.log(rho) - rho * tf.log(rho_hat) + (1 - rho) * tf.log(1 - rho) - (1 - rho) * tf.log(1 - rho_hat)\n",
    "\n",
    "    def regularization(self,weights):\n",
    "        return tf.nn.l2_loss(weights)\n",
    "\n",
    "    def loss(self,X):\n",
    "        H = self.encode(X)\n",
    "        rho_hat=tf.reduce_mean(H,axis=0)   #Average hidden layer over all data points in X, Page 14 in https://web.stanford.edu/class/cs294a/sparseAutoencoder_2011new.pdf\n",
    "        kl=self.kl_divergence(self.rho, rho_hat)\n",
    "        X_=self.decode(H)\n",
    "        diff=X-X_\n",
    "        cost= 0.5*tf.reduce_mean(tf.reduce_sum(diff**2,axis=1))  \\\n",
    "              +0.5*self.alpha*(tf.nn.l2_loss(self.W1) + tf.nn.l2_loss(self.W2))   \\\n",
    "              +self.beta*tf.reduce_sum(kl)\n",
    "        return cost\n",
    "\n",
    "    def training(self,training_data,  n_iter=100):\n",
    "\n",
    "        X=tf.placeholder(\"float\",shape=[None,training_data.shape[1]])\n",
    "        var_list=[self.W1,self.W2]\n",
    "        loss_=self.loss(X)\n",
    "        train_step=tf.contrib.opt.ScipyOptimizerInterface(loss_, var_list=var_list, method='L-BFGS-B',   options={'maxiter': n_iter})\n",
    "        train_step.minimize(self.sess, feed_dict={X: training_data})\n",
    "\n",
    "\n",
    "def visualizeW1(images, vis_patch_side, hid_patch_side, iter, file_name=\"trained_\"):\n",
    "    \"\"\" Visual all images in one pane\"\"\"\n",
    "\n",
    "    figure, axes = matplotlib.pyplot.subplots(nrows=hid_patch_side, ncols=hid_patch_side)\n",
    "    index = 0\n",
    "\n",
    "    for axis in axes.flat:\n",
    "        \"\"\" Add row of weights as an image to the plot \"\"\"\n",
    "\n",
    "        image = axis.imshow(images[index, :].reshape(vis_patch_side, vis_patch_side),\n",
    "                            cmap=matplotlib.pyplot.cm.gray, interpolation='nearest')\n",
    "        axis.set_frame_on(False)\n",
    "        axis.set_axis_off()\n",
    "        index += 1\n",
    "\n",
    "    \"\"\" Show the obtained plot \"\"\"\n",
    "    file=file_name+str(iter)+\".png\"\n",
    "    matplotlib.pyplot.savefig(file)\n",
    "    print(\"Written into \"+ file)\n",
    "    matplotlib.pyplot.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-ea63354a2a70>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
      "  Objective function value: 75.769005\n",
      "  Number of iterations: 11\n",
      "  Number of functions evaluations: 15\n",
      "Written into trained_4000.png\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    from tensorflow.examples.tutorials.mnist import input_data\n",
    "    mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "    n_inputs=784\n",
    "    n_hidden=10\n",
    "    start=0\n",
    "    lens=1000\n",
    "    learning_rate=0.1\n",
    "\n",
    "    sae=   FeedforwardSparseAutoEncoder(n_inputs,n_hidden)\n",
    "    n_iters=4000\n",
    "    sae.training(mnist.train.images[start:start+lens],n_iter=n_iters)\n",
    "\n",
    "    # After training the model, an image of the representations (W1) will be saved\n",
    "    # Please check trained4000.png for example\n",
    "  #  images=sae.W1.eval(sae.sess)\n",
    "  #  images=images.transpose()\n",
    " #   visualizeW1(images,28,10,n_iters)\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
